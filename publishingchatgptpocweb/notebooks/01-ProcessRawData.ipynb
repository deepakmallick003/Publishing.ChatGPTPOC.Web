{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eaad8d9-5451-41b6-ad36-eedb35e88f5a",
   "metadata": {},
   "source": [
    "# Process Publishing Raw Data to Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3f9973-613f-4b76-92ac-29705b7bfe0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3703cf2-c8ae-4cd2-aad7-341e737d2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_PATH = Path.cwd().parent\n",
    "if 'publishingchatgptpocweb' not in str(PARENT_PATH):\n",
    "    PARENT_PATH = PARENT_PATH / 'publishingchatgptpocweb'\n",
    "\n",
    "DATA_DIRECTORY = PARENT_PATH / 'data'\n",
    "STATIC_DIRECTORY = PARENT_PATH / 'static'\n",
    "\n",
    "JATS_DATA_DIRECTORY_PATH = DATA_DIRECTORY / 'raw'\n",
    "ARTICLES_DATA_DIRECTORY_PATH = DATA_DIRECTORY / 'processed' / 'articles'\n",
    "CONCEPTS_DATA_DIRECTORY_PATH = DATA_DIRECTORY / 'processed' / 'concepts'\n",
    "SPAQRQL_QUERY_FILE_PATH = STATIC_DIRECTORY / 'sparql' / 'sparql_query_template.sparql'\n",
    "CONCEPTS_NOT_FOUND_FILE_PATH = DATA_DIRECTORY / 'processed' / 'CONCEPTS_DETAILS_NOT_FOUND.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d68b52f-b38a-441a-85c6-c8412ff19c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_element(element, file, indent_level=0):\n",
    "    indent_unit = 4  # Define the number of spaces for each indentation level\n",
    "\n",
    "    def write_line(content, indent_level=0):\n",
    "        file.write(' ' * (indent_level * indent_unit) + content + '\\n')\n",
    "\n",
    "    if element.tag == \"article\":\n",
    "        write_line(\"Article:\", indent_level)\n",
    "\n",
    "    elif element.tag == \"article-id\":\n",
    "        attrib = element.attrib.get('pub-id-type')\n",
    "        text = element.text.strip()\n",
    "        if attrib == 'CABI-pan':\n",
    "            write_line(f\"PAN: {text}\", indent_level+1)\n",
    "        elif attrib == 'doi':\n",
    "            write_line(f\"DOI: {text}\", indent_level+1)\n",
    "            write_line(f\"Article Link/URL/Source: {'https://www.cabidigitallibrary.org/doi/' + text}\", indent_level+1)\n",
    "\n",
    "    elif element.tag == \"title-group\":\n",
    "        article_title = element.find('article-title')\n",
    "        if article_title is not None:\n",
    "            title_text = ''.join(article_title.itertext()).strip()\n",
    "            if title_text:\n",
    "                write_line(f\"Title: {title_text}\", indent_level+1)\n",
    "            \n",
    "    elif element.tag == \"abstract\":\n",
    "        abstract_summary = element.find('p')\n",
    "        if abstract_summary is not None:\n",
    "            summary_text = ''.join(abstract_summary.itertext()).strip()\n",
    "            if summary_text:\n",
    "                write_line(f\"Abstract Summary: {summary_text}\", indent_level+1)\n",
    "\n",
    "    elif element.tag == \"pub-date\":\n",
    "        write_line(\"Publishing Date:\", indent_level+1)\n",
    "        for child in element:\n",
    "            if child.text:\n",
    "                write_line(f\"{child.tag.capitalize()}: {child.text.strip()}\", indent_level + 2)\n",
    "\n",
    "    elif element.tag == \"isbn\" and element.text:\n",
    "        write_line(\"ISBN:\", indent_level+1)\n",
    "        write_line(element.text.strip(), indent_level + 2)\n",
    "\n",
    "    elif element.tag == \"publisher-name\" and element.text:\n",
    "        write_line(\"Publisher Name:\", indent_level+1)\n",
    "        write_line(element.text.strip(), indent_level + 2)\n",
    "\n",
    "    elif element.tag == \"publisher-loc\":\n",
    "        write_line(\"Publisher Location:\", indent_level+1)\n",
    "        for child in element:\n",
    "            if child.text:\n",
    "                write_line(f\"{child.tag.capitalize()}: {child.text.strip()}\", indent_level + 2)\n",
    "\n",
    "    elif element.tag == \"subj-group\" and element.attrib.get('subj-group-type') == \"cabi-codes\":\n",
    "        write_line(\"Subjects or Categories:\", indent_level+1)\n",
    "        for compound_subject in element.findall('compound-subject'):\n",
    "            labels = [label.text for label in compound_subject.findall('compound-subject-part[@content-type=\"label\"]') if label.text]\n",
    "            for label in labels:\n",
    "                write_line(f\"- {label.strip()}\", indent_level + 2)\n",
    "\n",
    "    elif element.tag == \"person-group\" and element.attrib.get('person-group-type') == \"author\":\n",
    "        author_names = []\n",
    "        for child in element:\n",
    "            name_parts = [sub_child.text for sub_child in child if sub_child.tag in ['given-names', 'surname'] and sub_child.text]\n",
    "            if name_parts:\n",
    "                author_names.append(' '.join(name_parts))\n",
    "        if author_names:\n",
    "            write_line(\"Authors:\", indent_level+1)\n",
    "            for name in author_names:\n",
    "                write_line(f\"- {name}\", indent_level + 2)\n",
    "\n",
    "    elif element.tag == \"kwd-group\" and element.attrib.get('kwd-group-type') == \"CABI-keyword\":\n",
    "        write_line(\"Keywords/Thesaurus Concepts:\", indent_level+1)\n",
    "        \n",
    "        vocab_map = {\n",
    "            'preferredTerm': \"Preferred Terms (Non-geographic, Non-organism contents from thesaurus):\",\n",
    "            'organismTerm': \"Organism Terms (Organism names from thesaurus):\",\n",
    "            'geographicTerm': \"Geographic Terms (Geographic Tags indicating content about a place):\"\n",
    "        }\n",
    "        terms = {key: [] for key in vocab_map}\n",
    "    \n",
    "        for kwd in element.findall('kwd'):\n",
    "            vocab = kwd.attrib.get('vocab')\n",
    "            if vocab and vocab in vocab_map and kwd.text:\n",
    "                terms[vocab].append(kwd.text.strip())\n",
    "\n",
    "        for vocab, description in vocab_map.items():\n",
    "            if terms[vocab]:\n",
    "                write_line(description, indent_level + 2)\n",
    "                for term in terms[vocab]:\n",
    "                    write_line(f\"- {term}\", indent_level + 3)\n",
    "\n",
    "    \n",
    "    for child in element:\n",
    "        parse_element(child, file, indent_level)\n",
    "\n",
    "\n",
    "def convert_jats_to_texts():\n",
    "    if not os.path.exists(ARTICLES_DATA_DIRECTORY_PATH):\n",
    "        os.makedirs(ARTICLES_DATA_DIRECTORY_PATH)\n",
    "    \n",
    "    for root_dir, sub_dirs, files in os.walk(JATS_DATA_DIRECTORY_PATH):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.xml'):\n",
    "                xm_file_path = os.path.join(root_dir, file_name)\n",
    "                tree = ET.parse(xm_file_path)\n",
    "                root = tree.getroot()\n",
    "              \n",
    "                output_text_file_path = os.path.join(ARTICLES_DATA_DIRECTORY_PATH, file_name.replace('.xml','.txt'))\n",
    "                with open(output_text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    parse_element(root, f)\n",
    "\n",
    "\n",
    "def fetch_concept_hierarchy(concept, query_template):\n",
    "    url = \"https://id.cabi.org/PoolParty/sparql/cabt\"\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    query = query_template.format(keyword=concept)\n",
    "    data = {'query': query, 'content-type': 'application/json-ld'}\n",
    "    response = requests.post(url, headers=headers, data=data)\n",
    "    \n",
    "    try:\n",
    "        response_json = response.json()\n",
    "        if not response_json.get('results', {}).get('bindings'):                        \n",
    "            return None\n",
    "        return response_json\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to decode JSON. Response text: {response.text}\")\n",
    "        return None\n",
    "        \n",
    "def extract_terms_from_xml(xml_file_path):\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    vocab_map = {\n",
    "        'preferredTerm': \"Preferred Terms (Non-geographic, Non-organism contents from thesaurus)\",\n",
    "        'organismTerm': \"Organism Terms (Organism names from thesaurus)\",\n",
    "        'geographicTerm': \"Geographic Terms (Geographic Tags indicating content about a place)\"\n",
    "    }\n",
    "\n",
    "    filtered_terms = []\n",
    "\n",
    "    kwd_group = root.find(\".//kwd-group[@kwd-group-type='CABI-keyword']\")\n",
    "    if kwd_group is not None:\n",
    "        for kwd in kwd_group.findall('kwd'):\n",
    "            vocab_type = kwd.get('vocab')\n",
    "            if vocab_type in vocab_map:\n",
    "                filtered_terms.append(kwd.text)\n",
    "\n",
    "    return filtered_terms\n",
    "\n",
    "def write_concept_to_file(output_text_file_path, concept_dict):      \n",
    "    # print('Creating new Concept document:', output_text_file_path)\n",
    "    with open(output_text_file_path, 'w', encoding='utf-8') as file:    \n",
    "        for key, value in concept_dict.items():\n",
    "            file.write(f\"{key}:\\n\")\n",
    "            for subkey, subvalue in value.items():\n",
    "                if isinstance(subvalue, list):\n",
    "                    file.write(f\"    {subkey}:\\n\")\n",
    "                    for item in subvalue:\n",
    "                        file.write(f\"        Concept:\\n\")\n",
    "                        for k, v in item.items():  # Changed this line\n",
    "                            file.write(f\"            {k}: {v}\\n\")\n",
    "                else:\n",
    "                    file.write(f\"    {subkey}:\\n\")\n",
    "                    for k, v in subvalue.items():\n",
    "                        file.write(f\"        {k}: {v}\\n\")\n",
    "\n",
    "\n",
    "def build_concept_dict(concept_name, concept_data):\n",
    "    concept_dict = {\n",
    "        \"Thesaurus Concept\": {\n",
    "            \"Concept\": None,\n",
    "            \"Broader Concept\": None,\n",
    "            \"Narrower Concepts\": set(),\n",
    "            \"Related Concepts\": set()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for binding in concept_data.get('results', {}).get('bindings', []):\n",
    "        # get all the necessary data\n",
    "        concept_uri = binding.get('concept', {}).get('value')\n",
    "        broader_name = binding.get('broaderLabel', {}).get('value')\n",
    "        broader_uri = binding.get('broaderConcept', {}).get('value')\n",
    "        narrower_name = binding.get('narrowerLabel', {}).get('value')\n",
    "        narrower_uri = binding.get('narrowerConcept', {}).get('value')\n",
    "        related_name = binding.get('relatedLabel', {}).get('value')\n",
    "        related_uri = binding.get('relatedConcept', {}).get('value')\n",
    "\n",
    "        # set the main concept\n",
    "        if concept_name:\n",
    "            concept_dict[\"Thesaurus Concept\"][\"Concept\"] = {\n",
    "                \"name\": concept_name,\n",
    "                \"uri\": binding.get('concept', {}).get('value')\n",
    "            }\n",
    "        \n",
    "        # set the broader concept\n",
    "        if broader_name and broader_uri:\n",
    "            concept_dict[\"Thesaurus Concept\"][\"Broader Concept\"] = {\n",
    "                \"name\": broader_name,\n",
    "                \"uri\": broader_uri\n",
    "            }\n",
    "\n",
    "        # add unique narrower concepts\n",
    "        if narrower_name and narrower_uri:\n",
    "            concept_dict[\"Thesaurus Concept\"][\"Narrower Concepts\"].add(\n",
    "                json.dumps({\"name\": narrower_name, \"uri\": narrower_uri})\n",
    "            )\n",
    "\n",
    "        # add unique related concepts\n",
    "        if related_name and related_uri:\n",
    "            concept_dict[\"Thesaurus Concept\"][\"Related Concepts\"].add(\n",
    "                json.dumps({\"name\": related_name, \"uri\": related_uri})\n",
    "            )\n",
    "\n",
    "    # convert sets to lists of dicts\n",
    "    concept_dict[\"Thesaurus Concept\"][\"Narrower Concepts\"] = [\n",
    "        json.loads(item) for item in concept_dict[\"Thesaurus Concept\"][\"Narrower Concepts\"]\n",
    "    ]\n",
    "    concept_dict[\"Thesaurus Concept\"][\"Related Concepts\"] = [\n",
    "        json.loads(item) for item in concept_dict[\"Thesaurus Concept\"][\"Related Concepts\"]\n",
    "    ]\n",
    "\n",
    "    return concept_dict\n",
    "\n",
    "\n",
    "def convert_concepts_to_texts():\n",
    "    if not os.path.exists(CONCEPTS_DATA_DIRECTORY_PATH):\n",
    "        os.makedirs(CONCEPTS_DATA_DIRECTORY_PATH)\n",
    "\n",
    "    with open(str(SPAQRQL_QUERY_FILE_PATH), 'r') as file:\n",
    "        query_template = file.read().replace('\\n', ' ').strip()\n",
    "    \n",
    "    concepts_set = set()\n",
    "\n",
    "    # Extracts all the unique concepts/keywords of ineterests from the JATS Directory XMLs\n",
    "    for root_dir, sub_dirs, files in os.walk(JATS_DATA_DIRECTORY_PATH):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.xml'):\n",
    "                xml_file_path = os.path.join(root_dir, file_name)\n",
    "                concepts = extract_terms_from_xml(xml_file_path)\n",
    "                concepts_set.update(concepts)\n",
    "\n",
    "    # Create documents for each concepts\n",
    "    for concept_name in concepts_set:\n",
    "        concept_file_name = concept_name.replace(' ', '_')\n",
    "        output_text_file_path = os.path.join(CONCEPTS_DATA_DIRECTORY_PATH, f'{concept_file_name}.txt')\n",
    "        # if not os.path.isfile(output_text_file_path):\n",
    "        concept_data = fetch_concept_hierarchy(concept_name, query_template)\n",
    "        if concept_data is not None:\n",
    "            concept_dict = build_concept_dict(concept_name, concept_data)\n",
    "            write_concept_to_file(output_text_file_path, concept_dict)\n",
    "        else:\n",
    "            # print(f\"No results found for the concept: {concept_name}\")\n",
    "            with open(str(CONCEPTS_NOT_FOUND_FILE_PATH), 'a', encoding='utf-8') as file:\n",
    "                file.write(concept_name + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a8ec9d3-5b4b-490c-93c9-719cf1bb9375",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_jats_to_texts()\n",
    "convert_concepts_to_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a3524-b96e-41c7-99eb-072a32baaf34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e7302-40ab-49f7-af99-a5a16dd6dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xml_file_path = os.path.join(JATS_DATA_DIRECTORY_PATH, '20203180098','20203180098.xml')\n",
    "# # tree = ET.parse(xm_file_path)\n",
    "# # root = tree.getroot()\n",
    "\n",
    "# # output_text_file_path = os.path.join(JATS_DATA_DIRECTORY_PATH, '20203180098', '20203180098.txt')\n",
    "# # with open(output_text_file_path, 'w', encoding='utf-8') as f:\n",
    "# #     parse_element(root, f)\n",
    "\n",
    "# concepts = extract_terms_from_xml(xml_file_path)\n",
    "# concepts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3f9973-613f-4b76-92ac-29705b7bfe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import openai\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import chromadb\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3703cf2-c8ae-4cd2-aad7-341e737d2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_PATH = Path.cwd().parent\n",
    "if 'publishingchatgptpocweb' not in str(PARENT_PATH):\n",
    "    PARENT_PATH = PARENT_PATH / 'publishingchatgptpocweb'\n",
    "\n",
    "DATA_DIRECTORY = PARENT_PATH / 'data'\n",
    "MODEL_DIRECTORY = PARENT_PATH / 'models'\n",
    "\n",
    "JATS_DATA_DIRECTORY_PATH = DATA_DIRECTORY / 'raw'\n",
    "ARTICLES_DATA_DIRECTORY_PATH = DATA_DIRECTORY / 'processed' / 'articles'\n",
    "CONCEPTS_DATA_DIRECTORY_PATH = DATA_DIRECTORY / 'processed' / 'concepts'\n",
    "CHROMA_DB_PUB = MODEL_DIRECTORY / 'langchain_chroma_db_pub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9516f82-987d-4350-9124-cdc1d83f2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_AI_API_SECRET = os.getenv('Open__AI__API__Secret')\n",
    "openai.api_key = OPEN_AI_API_SECRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d68b52f-b38a-441a-85c6-c8412ff19c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_article_metadata(document):\n",
    "    # Extracting various details from the page_content\n",
    "    title_match = re.search(r'Title:\\s*(.*?)(?:\\n|$)', document.page_content)\n",
    "    pan_match = re.search(r'PAN:\\s*(.*?)(?:\\n|$)', document.page_content)\n",
    "    source_match = re.search(r'Article Link/URL/Source:\\s*(.*?)(?:\\n|$)', document.page_content)\n",
    "    pub_date = re.search(r'Publishing Date:\\s*(.*?)(?:\\n|$)', document.page_content)    \n",
    "    isbn_match = re.search(r'ISBN:\\s*(.*?)(?:\\n|$)', document.page_content)\n",
    "    day_match = re.search(r'Day:\\s*(\\d{1,2})(?:\\n|$)', document.page_content)\n",
    "    month_match = re.search(r'Month:\\s*(\\d{1,2})(?:\\n|$)', document.page_content)\n",
    "    year_match = re.search(r'Year:\\s*(\\d{4})(?:\\n|$)', document.page_content)\n",
    "\n",
    "    document.metadata['document_type'] = 'article'\n",
    "    # Updating metadata dictionary\n",
    "    if title_match:\n",
    "        document.metadata['title'] = title_match.group(1)\n",
    "    if pan_match:\n",
    "        document.metadata['pan'] = pan_match.group(1)\n",
    "    if source_match:\n",
    "        document.metadata['source'] = source_match.group(1)\n",
    "    if isbn_match:\n",
    "        document.metadata['isbn'] = isbn_match.group(1)\n",
    "    if day_match and month_match and year_match:\n",
    "        pub_date = f\"{year_match.group(1)}-{month_match.group(1).zfill(2)}-{day_match.group(1).zfill(2)}\"\n",
    "        document.metadata['publishing_date'] = pub_date   \n",
    "    \n",
    "    return document\n",
    "\n",
    "def update_concepts_metadata(document):\n",
    "    # Extracting various details from the page_content\n",
    "    document.metadata['document_type'] = 'concept'\n",
    "    concept_section = re.search(r'Thesaurus Concept:\\n\\s*Concept:\\n(.*?)\\n(?:\\s*Broader Concept:|\\s*Narrower Concepts:|\\s*Related Concepts:|\\Z)', document.page_content, re.DOTALL)\n",
    "    if concept_section:\n",
    "        concept_details = concept_section.group(1)\n",
    "        name_match = re.search(r'name:\\s*(.*?)(?:\\n|$)', concept_details)\n",
    "        uri_match = re.search(r'uri:\\s*(.*?)(?:\\n|$)', concept_details)\n",
    "        # Updating metadata dictionary\n",
    "        if name_match:\n",
    "            document.metadata['name'] = name_match.group(1)\n",
    "            document.metadata['title'] = name_match.group(1)\n",
    "        if uri_match:\n",
    "            document.metadata['uri'] = uri_match.group(1)\n",
    "            document.metadata['source'] = uri_match.group(1)        \n",
    "    return document   \n",
    "\n",
    "\n",
    "def add_document_to_chroma_db(data_directory, type='article'):\n",
    "    for root_dir, sub_dirs, files in os.walk(data_directory):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.txt'):\n",
    "                try:\n",
    "                    text_file_path = os.path.join(root_dir, file_name)\n",
    "                    loader = TextLoader(text_file_path, encoding='utf8')\n",
    "                    documents = loader.load()\n",
    "                    if type=='articles':\n",
    "                        documents[0] = update_article_metadata(documents[0])\n",
    "                    elif type=='concepts':\n",
    "                        documents[0] = update_concepts_metadata(documents[0])\n",
    "                    instance.add_documents(documents)\n",
    "                except:\n",
    "                    print('error adding document', file_name)\n",
    "    return instance\n",
    "\n",
    "def create_vector_data_from_processed_documents():    \n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=OPEN_AI_API_SECRET)\n",
    "    instance = Chroma(embedding_function=embeddings, persist_directory=str(CHROMA_DB_PUB))\n",
    "    instance = add_document_to_chroma_db(ARTICLES_DATA_DIRECTORY_PATH, 'articles')\n",
    "    instance = add_document_to_chroma_db(CONCEPTS_DATA_DIRECTORY_PATH, 'concepts')\n",
    "    instance.persist()\n",
    "    instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe4618-3a91-4692-af7e-742da3573e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vector_data_from_processed_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05869635-ec9d-4f61-9122-8412429fed4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ad7ba-6f66-4452-82bc-aee8f7eb5617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc053d71-539e-4987-9b21-57519801aad0",
   "metadata": {},
   "source": [
    "## Test Chroma DB Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221acdd-6ff9-4f54-aa94-a106251c17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa(model='gpt-3.5-turbo'):\n",
    "    instance = Chroma(persist_directory=str(CHROMA_DB_PUB), embedding_function=OpenAIEmbeddings(openai_api_key=OPEN_AI_API_SECRET))\n",
    "\n",
    "    TEMPLATE = \"\"\"As EVA (Expert Virtual Assistance), your role is to help user with their queries related to articles, concepts etc. Please adhere to the following guidelines:\n",
    "    Utilize the given context (enclosed by <ctx></ctx>) to construct your responses:\n",
    "    ------\n",
    "    <ctx>\n",
    "    {context}\n",
    "    </ctx>\n",
    "    ------\n",
    "    \n",
    "    Q: {question}\n",
    "    A: \"\"\"\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "                template=TEMPLATE, \n",
    "                input_variables=[\"context\", \"question\"])        \n",
    "\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        streaming=True,\n",
    "        callbacks=[StreamingStdOutCallbackHandler()],\n",
    "        temperature=0,\n",
    "        openai_api_key=OPEN_AI_API_SECRET,\n",
    "    )\n",
    "    \n",
    "    retieval_qa = RetrievalQA.from_chain_type(\n",
    "                llm=llm,\n",
    "                retriever=instance.as_retriever(search_kwargs={\"k\": 1}),\n",
    "                chain_type_kwargs={\n",
    "                    \"verbose\": False,\n",
    "                    \"prompt\": prompt_template\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return retieval_qa\n",
    "\n",
    "def get_response(qa, request):\n",
    "    response = qa.run(text)\n",
    "    return response\n",
    "\n",
    "model = 'gpt-3.5-turbo'\n",
    "# model = 'gpt-3.5-turbo-16k'\n",
    "# model = 'gpt-4'\n",
    "qa = load_qa(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbd75d-ced9-4c35-8f76-74c2aa52ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text='Can you tell me a more about the Tropical Timber Market Report?'\n",
    "# text='Tell me the articles names and pan where CABI-keyword preferredTerm is world markets?'\n",
    "# text='When was this article related to Tropical Timber Market Report published? and who wrote it in terms of authors?'\n",
    "text='who wrote The ebola epidemic in West Africa: proceedings of a workshop ?'\n",
    "\n",
    "get_response(qa, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
